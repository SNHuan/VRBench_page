# Abstract

Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics.
Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation?
Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for **spatial reasoning**.
In this work, we explore the reasoning via video paradigm and introduce **VR-Bench**—a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities.
Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, 
VR-Bench contains 7,920 procedurally generated videos across **five maze types and diverse visual styles**.
Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model.
% In spatial reasoning task, Video model outperforms leading VLMs and generalize well across diverse scenarios, tasks, and levels of complexity.
Video models exhibit stronger **spatial perception** during reasoning, outperforming leading VLMs and **generalizing well** across diverse scenarios, tasks, and levels of complexity.
We further discover a **test-time scaling** effect, where diverse sampling during inference improves reasoning reliability by 10–20\%.
These findings highlight the unique potential and scalability of **reasoning via video** for spatial reasoning tasks.